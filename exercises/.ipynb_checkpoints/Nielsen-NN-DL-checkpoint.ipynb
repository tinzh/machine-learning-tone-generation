{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commits to GitHub\n",
    "### Save and add a message before committing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd /Users/asianzhang/Documents/GitHub/machine-learning-tone-generation\n",
    "git add .\n",
    "git commit --allow-empty-message -m \"Changed more display and spectrogram manipulation methods\"\n",
    "git push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "---\n",
    "## Chapter 1\n",
    "___\n",
    "### Sigmoid neurons simulating perceptrons, part I\n",
    "Given that $z=w\\cdot x+b$, only $\\mbox{sgn}(z)$ affects the output of the perceptron. If $w$ and $b$ are multiplied by $c>0$, then $z=(cw)\\cdot x+ (cb)=c(w\\cdot x+b$). Since the multiplication of a number by a positive constant does not change the $ \\mbox{sgn}()$ of the number, the multiplication by $c$ will not affect the output of the perceptron.\n",
    "### Sigmoid neurons simulating perceptrons, part II \n",
    "As established in part I, multiplication of $w$ and $b$ by a constant $c>0$ will cause $z$ to be multiplied by the constant $c$. So, as $c\\to\\infty$, $|z|\\to\\infty$, and $\\lim\\limits_{z\\to-\\infty}\\sigma(z)=0$ and $\\lim\\limits_{z\\to\\infty}\\sigma(z)=1$. Since the output is either 0 or 1, this behavior models a perceptron. However, if $w\\cdot x+b = z = 0$, then $\\lim\\limits_{c\\to\\infty}cz=0$, and $\\sigma(z=0)=0.5$, which is not possible with a perceptron.\n",
    "\n",
    "---\n",
    "### Designing an output layer for bitwise representation of prediction\n",
    "$$ \n",
    "Weights = \\left[ {\\begin{array}{cccccccccc} \n",
    "            0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 9 & 9\\\\\n",
    "            0 & 0 & 0 & 0 & 9 & 9 & 9 & 9 & 0 & 0\\\\\n",
    "            0 & 0 & 9 & 9 & 0 & 0 & 9 & 9 & 0 & 0\\\\\n",
    "            0 & 9 & 0 & 9 & 0 & 9 & 0 & 9 & 0 & 9\\\\\n",
    "            \\end{array} } \\right],\\ \n",
    "Biases = \\left[ {\\begin{array}{c}\n",
    "            1\\\\\n",
    "            1\\\\\n",
    "            1\\\\\n",
    "            1\\\\\n",
    "            \\end{array} } \\right]\n",
    "$$\n",
    "\n",
    "---\n",
    "### Prove the following assertion:\n",
    "Assertion: The choice of $\\Delta v$ which minimizes $\\nabla C \\cdot \\Delta v  \\approx \\Delta C$ is $\\Delta v = -\\eta \\nabla C$ (where $\\eta = \\epsilon / \\|\\nabla C\\|$ is determined by the size constraint $\\|\\Delta v\\| = \\epsilon$, for some small fixed $\\epsilon>0$).\n",
    "\n",
    "According to the Cauchy-Schwarz inequality, $|\\nabla C \\cdot \\Delta v| \\leq \\|\\nabla C \\|\\\n",
    "\\|\\Delta v\\|$. Since there is a constraint $\\|\\Delta v\\|=\\epsilon$, $|\\nabla C \\cdot \\Delta v| \\leq \\epsilon\\|\\nabla C \\|$. Since $\\nabla C$ and $\\epsilon$ are constant, $\\epsilon\\|\\nabla C \\|$ is also constant. Also, since the absolute value is used in the inequality, the minimum for $\\nabla C \\cdot \\Delta v$ will be negative, leading to $\\nabla C\\cdot\\Delta v \\geq -\\epsilon\\|\\nabla C\\|$. Therefore, $\\mbox{min}(\\nabla C\\cdot\\Delta v) = \\epsilon\\|\\nabla C\\|$. $\\Delta v = -\\eta\\nabla C$ satisfies this equation, and is thus the minimum of $\\nabla C \\cdot\\Delta v$. \n",
    "\n",
    "($\\nabla C \\cdot\\Delta v=\\nabla C\\cdot(-\\eta\\nabla C)=-\\eta(\\nabla C\\cdot\\nabla C)=-\\eta\\|\\nabla C\\|^2=-\\frac{\\epsilon}{\\|\\nabla C\\|}\\|\\nabla C\\|^2=-\\epsilon\\|\\nabla C\\|$)\n",
    "### Gradient descent when $C$ is a function of just one variable\n",
    "In this case, $C$ is one-half of the squared residual between the output of a function and the expected value of the function (for confimation, see equation 6). Minimizing C leads to a smaller residual, optimizing the the function to output the desired value with the given input. The geometric interpretation is similar, with a \"ball\" ($C$) going down towards the lowest point of a hill where error is minimized.\n",
    "\n",
    "---\n",
    "### Naming one advantage and one disadvantage of minibatch size of 1 (online learning) compared to a size of 20 in stochastic gradient descent\n",
    "Advantage: Less storage, i.e. no need to store $\\nabla C$ during a minibatch  \n",
    "Disadvantage: Learning may be erratic, ex. outliers will have a very high cost, leading to training in a potentially wrong direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
