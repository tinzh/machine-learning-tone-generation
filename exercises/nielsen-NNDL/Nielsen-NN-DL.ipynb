{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commits to GitHub\n",
    "# Save and add a message before committing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 23bcdac] Organized files\n",
      " 3 files changed, 484 insertions(+), 20 deletions(-)\n",
      " create mode 100644 exercises/nielsen-NNDL/.ipynb_checkpoints/Nielsen-NN-DL-checkpoint.ipynb\n",
      " rename exercises/{ => nielsen-NNDL}/Nielsen-NN-DL.ipynb (98%)\n",
      " rename exercises/{ => nielsen-NNDL}/mnist.pkl.gz (100%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://github.com/asianzhang/machine-learning-tone-generation.git\n",
      "   f734331..23bcdac  master -> master\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "cd /Users/asianzhang/Documents/GitHub/machine-learning-tone-generation\n",
    "git add .\n",
    "git commit --allow-empty-message -m \"Organized files\"\n",
    "git push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "---\n",
    "# Chapter 1\n",
    "___\n",
    "## Sigmoid neurons\n",
    "---\n",
    "### Sigmoid neurons simulating perceptrons, part I\n",
    "Given that $z=w\\cdot x+b$, only $\\mbox{sgn}(z)$ affects the output of the perceptron. If $w$ and $b$ are multiplied by $c>0$, then $z=(cw)\\cdot x+ (cb)=c(w\\cdot x+b$). Since the multiplication of a number by a positive constant does not change the $ \\mbox{sgn}()$ of the number, the multiplication by $c$ will not affect the output of the perceptron.\n",
    "### Sigmoid neurons simulating perceptrons, part II \n",
    "As established in part I, multiplication of $w$ and $b$ by a constant $c>0$ will cause $z$ to be multiplied by the constant $c$. So, as $c\\to\\infty$, $|z|\\to\\infty$, and $\\lim\\limits_{z\\to-\\infty}\\sigma(z)=0$ and $\\lim\\limits_{z\\to\\infty}\\sigma(z)=1$. Since the output is either 0 or 1, this behavior models a perceptron. However, if $w\\cdot x+b = z = 0$, then $\\lim\\limits_{c\\to\\infty}cz=0$, and $\\sigma(z=0)=0.5$, which is not possible with a perceptron.\n",
    "\n",
    "\n",
    "## A simple network to classify handwritten digits\n",
    "---\n",
    "### Designing an output layer for bitwise representation of prediction\n",
    "$$ \n",
    "Weights = \\left[ {\\begin{array}{cccccccccc} \n",
    "            0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 99 & 99\\\\\n",
    "            0 & 0 & 0 & 0 & 99 & 99 & 99 & 99 & 0 & 0\\\\\n",
    "            0 & 0 & 99 & 99 & 0 & 0 & 99 & 99 & 0 & 0\\\\\n",
    "            0 & 99 & 0 & 99 & 0 & 99 & 0 & 99 & 0 & 99\\\\\n",
    "            \\end{array} } \\right],\\ \n",
    "Biases = \\left[ {\\begin{array}{c}\n",
    "            0\\\\\n",
    "            0\\\\\n",
    "            0\\\\\n",
    "            0\\\\\n",
    "            \\end{array} } \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "## Learning with gradient descent\n",
    "---\n",
    "### Prove the following assertion:\n",
    "Assertion: The choice of $\\Delta v$ which minimizes $\\nabla C \\cdot \\Delta v  \\approx \\Delta C$ is $\\Delta v = -\\eta \\nabla C$ (where $\\eta = \\epsilon / \\|\\nabla C\\|$ is determined by the size constraint $\\|\\Delta v\\| = \\epsilon$, for some small fixed $\\epsilon>0$).\n",
    "\n",
    "According to the Cauchy-Schwarz inequality, $|\\nabla C \\cdot \\Delta v| \\leq \\|\\nabla C \\|\\\n",
    "\\|\\Delta v\\|$. Since there is a constraint $\\|\\Delta v\\|=\\epsilon$, $|\\nabla C \\cdot \\Delta v| \\leq \\epsilon\\|\\nabla C \\|$. Since $\\nabla C$ and $\\epsilon$ are constant, $\\epsilon\\|\\nabla C \\|$ is also constant. Also, since the absolute value is used in the inequality, the minimum for $\\nabla C \\cdot \\Delta v$ will be negative, leading to $\\nabla C\\cdot\\Delta v \\geq -\\epsilon\\|\\nabla C\\|$. Therefore, $\\mbox{min}(\\nabla C\\cdot\\Delta v) = \\epsilon\\|\\nabla C\\|$. $\\Delta v = -\\eta\\nabla C$ satisfies this equation, and is thus the minimum of $\\nabla C \\cdot\\Delta v$. \n",
    "\n",
    "($\\nabla C \\cdot\\Delta v=\\nabla C\\cdot(-\\eta\\nabla C)=-\\eta(\\nabla C\\cdot\\nabla C)=-\\eta\\|\\nabla C\\|^2=-\\frac{\\epsilon}{\\|\\nabla C\\|}\\|\\nabla C\\|^2=-\\epsilon\\|\\nabla C\\|$)\n",
    "### Gradient descent when $C$ is a function of just one variable\n",
    "In this case, $C$ is one-half of the squared residual between the output of a function and the expected value of the function (for confimation, see equation 6). Minimizing C leads to a smaller residual, optimizing the the function to output the desired value with the given input. The geometric interpretation is similar, with a \"ball\" ($C$) going down towards the lowest point of a hill where error is minimized.\n",
    "### Naming one advantage and one disadvantage of minibatch size of 1 (online learning) compared to a size of 20 in stochastic gradient descent\n",
    "Advantage: Less storage, i.e. no need to store $\\nabla C$ during a minibatch  \n",
    "Disadvantage: Learning may be erratic, ex. outliers will have a very high cost, leading to training in a potentially wrong direction\n",
    "\n",
    "\n",
    "## Implementing our network to classify digits\n",
    "---\n",
    "### Verifying the use of matrices with component form\n",
    "Given $a' = \\sigma(wa+b)$:  \n",
    "Since $w$ and $a$ are matrices, using the definition of matrix multiplication,\n",
    "$a_j = \\sigma((\\sum_k w_{jk} a_k) + b_j)$, which is consistent with the rule for computing the output of a sigmoid neuron.\n",
    "### Creating a network with just two layers\n",
    "Max classification accuracy: 91.88%\n",
    "# Chapter 2\n",
    "---\n",
    "## Proof of the four fundamental equations\n",
    "---\n",
    "### Proving equations BP3 and BP4\n",
    "BP3: $\\frac{\\partial C}{\\partial b^l_j}=\\delta^l_j$  \n",
    "Proof: $\\frac{\\partial C}{\\partial b^l_j} = \\frac{\\partial C}{\\partial z^l_j}\\frac{\\partial z^l_j}{\\partial b^l_j}=\\delta^1_j(1)=\\delta^1_j$\n",
    "\n",
    "BP4: $\\frac{\\partial C}{\\partial w^l_{jk}}=a^{l-1}_k\\delta^l_j$  \n",
    "Proof: $\\frac{\\partial C}{\\partial w^l_{jk}}=\\frac{\\partial C}{\\partial z^l_j}\\frac{\\partial z^l_j}{\\partial w^l_{jk}}=\\delta^l_ja^{l-1}_k=a^{l-1}_k\\delta^l_j$\n",
    "## The backpropagation algorithm\n",
    "---\n",
    "### Backpropagation with a single modified neuron\n",
    "Change BP2 so that $\\delta^l=((w^{l+1})^T\\delta^{l+1})\\odot f'(z^l)$ instead of $((w^{l+1})^T\\delta^{l+1})\\odot \\sigma'(z^l)$\n",
    "### Backpropagation with linear neurons\n",
    "Replace all $\\sigma'(z^l)$ in the backpropagation algorithm to $1$ (i.e. remove all appearances of $\\sigma'(z^l)$).\n",
    "# Chapter 3\n",
    "---\n",
    "## The cross-entropy cost function\n",
    "---\n",
    "### Verifying the derivative of the sigmoid function\n",
    "$\\sigma(z)=\\frac{1}{1+e^{-z}}=(1+e^{-z})^{-1}$  \n",
    "$\\sigma'(z)=-(1+e^{-z})^{-2}(-e^{-z})=\\frac{1}{1+e^{-z}}\\frac{e^{-z}}{1+e^{-z}}=\\frac{1}{1+e^{-z}}\\frac{(1+e^{-z})-(1)}{1+e^{-z}}=\\frac{1}{1+e^{-z}}(\\frac{1+e^{-z}}{1+e^{-z}}-\\frac{(1)}{1+e^{-z}})=\\sigma(z)(1-\\sigma(z))$\n",
    "### Mixing up $y$ and $a$ in the cross entropy function\n",
    "In $-[a \\ln y + (1-a) \\ln (1-y)]$, if $y$ (the expected value for the neuron) is $0$ or $1$, there is a $\\ln 0$, which is undefined. Since $a$ is never exactly $0$ or $1$, in $-[y \\ln a  + (1-y) \\ln (1-a)]$, there will never be $\\ln 0$.\n",
    "### Verifying the use of cross-entropy for cost, $y\\neq 0$ or $1$\n",
    "$a=\\sigma(z), C = -\\frac{1}{n} \\sum_x \\left[y \\ln a + (1-y ) \\ln (1-a) \\right]$  \n",
    "\n",
    "$\\frac{\\partial C}{\\partial a}=-\\frac{1}{n}\\sum_x \\left[\\frac{y}{a}-\\frac{1-y}{1-a}\\right]$\n",
    "\n",
    "When $\\sigma(z)=a=y$, $\\frac{\\partial C}{\\partial a}$ is 0, meaning that the cross-engtropy is minimized at $\\sigma(z)=a=y$ for all $y$.\n",
    "### Showing that sigmoid output activations won't always sum to $1$\n",
    "Consider a network with an output layer of one neuron. Since the output is solely within the range of the sigmoid function $(0,1)$, the output activation will never sum to $1$. In fact, even with a network with a multi-neuron output layer, having the sum of the activations equal $1$ is impossible due to the continuity of the sigmoid function.\n",
    "### Monotonicity of softmax\n",
    "$a^L_j=\\frac{e^{z^L_j}}{\\sum_k e^{z^L_k}}$  \n",
    "$\\frac{\\partial a^L_j}{\\partial z^L_{k=j}}=\\frac{e^{z^L_j}}{\\sum_k e^{z^L_k}}$  \n",
    "$\\frac{\\partial a^L_j}{\\partial z^L_{k\\neq j}}=\\frac{-e^{z^L_j}e^{z^L_k}}{\\left(\\sum_k e^{z^L_k}\\right)^2}$  \n",
    "Since $e^x>0$ for all real $x$, $\\frac{\\partial a^L_j}{\\partial z^L_{k}}$ is positive if $j=k$ and negative if $j\\neq k$.\n",
    "### Non-locality of softmax\n",
    "Since in the denominator of the softmax equation all the weighted inputs are summed over, any particular output activation $a^L_j$ depends on all the weighted inputs.\n",
    "## Overfitting and regularization\n",
    "---\n",
    "### Dangers of \"over-regularization\"\n",
    "If arbitrarily large rotations of training images are used, then one digit might look like another. This is most notable with 6 and 9, which are only different from each other by a 180Â° rotation.\n",
    "## Weight initialization\n",
    "---\n",
    "### Verifying the standard deviation of $z$ with new initialization method\n",
    "$\\sigma^2_w=\\frac{1}{n_{in}}$  \n",
    "$\\sigma^2_{neuron}=n_{in=1}\\sigma^2_w+\\sigma^2_b=\\frac{n_{in}}{2}\\frac{1}{n_{in}}+1=\\frac{3}{2}$  \n",
    "$\\sigma_{neuron}=\\sqrt{\\frac{3}{2}}$\n",
    "## How to choose a network's hyper-parameters\n",
    "---\n",
    "### Obstacles to using gradient descent for $\\lambda$ and $\\eta$\n",
    "For both $\\lambda$ and $\\eta$, gradient descent would require a method that simulates the training of a network. Since a network starts with random parameters, the function that gradient descent would be performed on needs to encompass all possible starting positions, which is computationally impossible.\n",
    "### Using $\\mu>1$ or $\\mu<0$ in the momentum technique\n",
    "If $\\mu>1$, the velocity would grow exponentially, especially towards the end of learning when the gradient descent is small. If $\\mu<0$, gradient descent would oscillate between negative and positive and defeat the purpose of building momentum, as the momentum would change constantly.\n",
    "### Proving the relationship between $\\sigma$ and $\\tanh$\n",
    "$\\tanh(z)=\\frac{e^z-e^{-z}}{e^z+e^{-z}}\\left(\\frac{e^{-z}}{e^{-z}}\\right)=\\frac{1-e^{-2z}}{1+e^{-2z}}=\\frac{2}{1+e^{-2z}}-\\frac{1+e^{-2z}}{1+e^{-2z}}=2\\sigma(2z)-1$  \n",
    "  \n",
    "$\\sigma(2z)=\\frac{1+\\tanh(z)}{2}$  \n",
    "  \n",
    "$\\sigma(z)=\\frac{1+\\tanh(z/2)}{2}$\n",
    "## Unfinished exercises\n",
    "---  \n",
    "Look more at regularization techniques (ex. dropout)\n",
    "# Chapter 4\n",
    "---\n",
    "## No exercises in this chapter.\n",
    "# Chapter 5\n",
    "---\n",
    "## Cause of the vanishing gradient problem\n",
    "---\n",
    "### Possibility of the use of a different activation function with a larger derivative\n",
    "This still wouldn't solve the unstable gradient problem; the weights would still affect the network in the $w\\sigma'(z)$ terms, and above a below a certain threshold for the weights, the gradient still either vanishes or explodes exponentially similar to before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems\n",
    "---\n",
    "## Chapter 2\n",
    "---\n",
    "### Alternate presentation of the equations of backpropagation\n",
    "###### BP1\n",
    "Since $\\sum'(z^L)$ is populated only on the diagonals, $\\delta^L_j = (\\sum'(z^L)_{jj}=\\sigma'(z^L)_j)\\nabla_aC_j=(\\nabla_aC\\odot\\sigma'(z^L))_j$\n",
    "###### BP2\n",
    "Using similar logic as to BP1 above, $\\delta^l=\\sum'(z^l)(w^{l+1})^T\\delta^{l+1}=\\sum'(z^l)((w^{l+1})^T\\delta^{l+1})=((w^{l+1})^T\\delta^{l+1})\\odot\\sigma'(z^l)$\n",
    "###### Combining BP1 and BP2\n",
    "$\\delta^l = \\Sigma'(z^l) (w^{l+1})^T \\ldots \\Sigma'(z^{L-1}) (w^L)^T \n",
    "    \\Sigma'(z^L) \\nabla_a C$ can be obtained by chaining together $\\delta^l$ with $\\delta^l+1$ etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# modification of network.py to implement fully matrix-based\n",
    "# approach to backpropagation over a mini-batch\n",
    "\n",
    "import numpy as np\n",
    "import types\n",
    "\n",
    "def sigmoid(z):\n",
    "    return .5 * (1 + np.tanh(.5 * z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "class SelfNetwork2(object):\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "        if(callable(args[-1]) and callable(args[-2])):\n",
    "            self.activation = args[-2]\n",
    "            self.dadz = args[-1]\n",
    "            args = args[:-2]\n",
    "        else:\n",
    "            self.activation = sigmoid\n",
    "            self.dadz = sigmoid_prime\n",
    "        if(isinstance(args[0], (list,))):\n",
    "            sizes = args[0]\n",
    "        else:\n",
    "            sizes = args\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = np.asarray([np.random.randn(y, 1) for y in sizes[1:]])\n",
    "        self.weights = np.asarray([np.random.randn(y, x)/np.sqrt(x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])])\n",
    "        self.velocity = np.asarray([np.asarray([np.zeros((y, x))/np.sqrt(x)\n",
    "                           for x, y in zip(sizes[:-1], sizes[1:])]), \n",
    "                                    np.asarray([np.zeros((y, 1)) for y in sizes[1:]])])\n",
    "        # Create a Network object net with 2 neurons in the first layer,\n",
    "        # 3 neurons in the second layer, and 1 neuron in the final\n",
    "        # layer, do\n",
    "        # net = Network([2, 3, 1])\n",
    "    \n",
    "    def setActivationFunction(self, func, dfunc):\n",
    "        self.activation = func\n",
    "        self.dadz = dfunc\n",
    "        \n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if \"a\" is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = self.activation(w @ a + b)\n",
    "        return a\n",
    "    \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None, \n",
    "            output=\"tuple\", quiet=False, cost='cross-entropy', lmbda=5.0, \n",
    "            regularization=\"L2\", schedule=None, momentum=None):\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        if(schedule==None or test_data==None):\n",
    "            for j in range(1, epochs+1):\n",
    "                np.random.shuffle(training_data)\n",
    "                mini_batches = [\n",
    "                    training_data[k:k+mini_batch_size]\n",
    "                    for k in range(0, n, mini_batch_size)]\n",
    "                for mini_batch in mini_batches:\n",
    "                    self.update_mini_batch(mini_batch, eta, cost, lmbda, n, \n",
    "                                           regularization, momentum)\n",
    "                if(not(quiet)):\n",
    "                    if test_data:\n",
    "                        print(\"Epoch {0}: {1} / {2}\".format(\n",
    "                            j, self.evaluate(test_data), n_test))\n",
    "                    else:\n",
    "                        print (\"Epoch {0} complete\".format(j))\n",
    "        else:\n",
    "            epoch = 0\n",
    "            for j in range(1, 8):\n",
    "                val_accs = [0, []]\n",
    "                for i in range(0, schedule):\n",
    "                    np.random.shuffle(training_data)\n",
    "                    mini_batches = [\n",
    "                        training_data[k:k+mini_batch_size]\n",
    "                        for k in range(0, n, mini_batch_size)]\n",
    "                    for mini_batch in mini_batches:\n",
    "                        self.update_mini_batch(mini_batch, eta, cost, lmbda, n, \n",
    "                                               regularization, momentum)\n",
    "                    val_accs[1].append(self.evaluate(test_data))\n",
    "                    epoch += 1\n",
    "                    if(not(quiet)):\n",
    "                        print(\"Epoch {0}: {1} / {2}, Eta: {3}\".format(\n",
    "                                epoch, val_accs[1][-1], n_test, eta))\n",
    "                while True:\n",
    "                    np.random.shuffle(training_data)\n",
    "                    mini_batches = [\n",
    "                        training_data[k:k+mini_batch_size]\n",
    "                        for k in range(0, n, mini_batch_size)]\n",
    "                    for mini_batch in mini_batches:\n",
    "                        self.update_mini_batch(mini_batch, eta, cost, lmbda, n, \n",
    "                                               regularization, momentum)\n",
    "                    acc = self.evaluate(test_data)\n",
    "                    epoch += 1\n",
    "                    if(not(quiet)):\n",
    "                        print(\"Epoch {0}: {1} / {2}, Eta: {3}\".format(\n",
    "                                epoch, val_accs[1][-1], n_test, eta))\n",
    "                    if(val_accs[1][val_accs[0]]>=acc):\n",
    "                        eta /= 2\n",
    "                        break\n",
    "                    else:\n",
    "                        val_accs[1][val_accs[0]] = acc\n",
    "                        val_accs[0] += 1\n",
    "                        if(val_accs[0]==schedule):\n",
    "                            val_accs[0]=0\n",
    "\n",
    "        if test_data:\n",
    "            if(output==\"tuple\"):\n",
    "                out = (self.evaluate(test_data), n_test)\n",
    "                print(\"Training finished. Final classification accuracy: {0}/{1}\".format(\n",
    "                    out[0], out[1]))\n",
    "            if(output==\"percent\"):\n",
    "                out = self.evaluate(test_data)/n_test\n",
    "                print(\"Training finished. Final classification accuracy: {0}%\".format(\n",
    "                    out*100))\n",
    "            return out\n",
    "    \n",
    "    def update_mini_batch(self, mini_batch, eta, cost, lmbda, n, regularization, mu):\n",
    "        nabla_b = np.array([np.zeros(b.shape) for b in self.biases])\n",
    "        nabla_w = np.array([np.zeros(w.shape) for w in self.weights])\n",
    "        mini_batch = np.array(mini_batch)\n",
    "        mini_batch_size = mini_batch.size/2.0\n",
    "        images = np.concatenate(mini_batch[:,0], axis=1)\n",
    "        outputs = np.concatenate(mini_batch[:,1], axis=1)\n",
    "        zs = []\n",
    "        activations = [images]\n",
    "        activation = images\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            zs.append(w @ activations[-1] + b)\n",
    "            activations.append(self.activation(zs[-1]))\n",
    "        if(cost==\"quadratic\"):\n",
    "            delta = self.dCda(activations[-1],outputs)*self.dadz(zs[-1])\n",
    "        if(cost==\"cross-entropy\"):\n",
    "            delta = activations[-1] - outputs\n",
    "        nabla_b[-1] = np.sum(delta, axis=1)*eta/mini_batch_size\n",
    "        nabla_w[-1] = (delta @ activations[-2].T)*eta/mini_batch_size\n",
    "        for l in range(2, self.num_layers):\n",
    "            delta = (self.weights[-l+1].T @ delta)*self.dadz(zs[-l])\n",
    "            nabla_b[-l] = np.sum(delta, axis=1)*eta/mini_batch_size\n",
    "            nabla_w[-l] = (delta @ activations[-l-1].T)*eta/mini_batch_size\n",
    "        # The following basically changes the weights and biases by the mean of the nablas\n",
    "        if mu:\n",
    "            if(regularization==\"L2\"):\n",
    "                self.velocity = [[mu*v-eta*lmbda*w/n-nw \n",
    "                                  for v, w, nw in zip(self.velocity[0], \n",
    "                                                      self.weights, nabla_w)], \n",
    "                                 [(mu*v-nb.reshape(nb.shape[0],1))\n",
    "                                  for v, nb in zip(self.velocity[1], nabla_b)]]\n",
    "            elif(regularization==\"L1\"):\n",
    "                self.velocity = [mu*v-eta*lmbda/n*np.sign(w)-nw \n",
    "                                 for v, w, nw in zip(self.velocity[0], self.weights, nabla_w)], \n",
    "                [(mu*v-nb.reshape(nb.shape[0],1)) for v,nb in zip(self.velocity[1], nabla_b)]\n",
    "            else:\n",
    "                self.velocity = [[mu*v-nw for nw in nabla_w], \n",
    "                                                     [(mu*v-nb.reshape(nb.shape[0],1)) \n",
    "                                                      for v, nb in zip(self.velocity[1], nabla_b)]]\n",
    "            self.weights = [w+v for w,v in zip(self.weights, self.velocity[0])]\n",
    "            self.biases = [b+v for b,v in zip(self.biases, self.velocity[1])]\n",
    "        else:\n",
    "            if(regularization==\"L2\"):\n",
    "                self.weights = [(1-eta*lmbda/n)*w-nw for w, nw in zip(self.weights, nabla_w)]\n",
    "                self.biases = ([(b-nb.reshape(nb.shape[0],1))\n",
    "                               for b, nb in zip(self.biases, nabla_b)])\n",
    "            elif(regularization==\"L1\"):\n",
    "                self.weights = ([w-eta*lmbda/n*np.sign(w)-nw \n",
    "                                for w, nw in zip(self.weights, nabla_w)])\n",
    "                self.biases = ([(b-nb.reshape(nb.shape[0],1))\n",
    "                               for b, nb in zip(self.biases, nabla_b)])\n",
    "            else:\n",
    "                self.weights = [w-nw for w, nw in zip(self.weights, nabla_w)]\n",
    "                self.biases = ([(b-nb.reshape(nb.shape[0],1))\n",
    "                               for b, nb in zip(self.biases, nabla_b)])\n",
    "    \n",
    "    def dCda(self, output_activations, y):\n",
    "        return (output_activations-y)\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        #print(self.feedforward(test_data[0][0]).shape)\n",
    "        return sum(int(x==y) for (x, y) in test_results)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training networks\n",
      "Epoch 1: 9254 / 10000, Eta: 1.5\n",
      "Epoch 2: 9297 / 10000, Eta: 1.5\n",
      "Epoch 3: 9292 / 10000, Eta: 1.5\n",
      "Epoch 4: 9189 / 10000, Eta: 1.5\n",
      "Epoch 5: 9263 / 10000, Eta: 1.5\n",
      "Epoch 6: 9326 / 10000, Eta: 1.5\n",
      "Epoch 7: 9371 / 10000, Eta: 1.5\n",
      "Epoch 8: 9239 / 10000, Eta: 1.5\n",
      "Epoch 9: 9226 / 10000, Eta: 1.5\n",
      "Epoch 10: 9419 / 10000, Eta: 1.5\n",
      "Epoch 11: 9419 / 10000, Eta: 1.5\n",
      "Epoch 12: 9494 / 10000, Eta: 0.75\n",
      "Epoch 13: 9584 / 10000, Eta: 0.75\n",
      "Epoch 14: 9592 / 10000, Eta: 0.75\n",
      "Epoch 15: 9660 / 10000, Eta: 0.75\n",
      "Epoch 16: 9613 / 10000, Eta: 0.75\n",
      "Epoch 17: 9597 / 10000, Eta: 0.75\n",
      "Epoch 18: 9611 / 10000, Eta: 0.75\n",
      "Epoch 19: 9508 / 10000, Eta: 0.75\n",
      "Epoch 20: 9596 / 10000, Eta: 0.75\n",
      "Epoch 21: 9627 / 10000, Eta: 0.75\n",
      "Epoch 22: 9627 / 10000, Eta: 0.75\n",
      "Epoch 23: 9627 / 10000, Eta: 0.75\n",
      "Epoch 24: 9713 / 10000, Eta: 0.375\n",
      "Epoch 25: 9719 / 10000, Eta: 0.375\n",
      "Epoch 26: 9740 / 10000, Eta: 0.375\n",
      "Epoch 27: 9678 / 10000, Eta: 0.375\n",
      "Epoch 28: 9699 / 10000, Eta: 0.375\n",
      "Epoch 29: 9670 / 10000, Eta: 0.375\n",
      "Epoch 30: 9719 / 10000, Eta: 0.375\n",
      "Epoch 31: 9680 / 10000, Eta: 0.375\n",
      "Epoch 32: 9703 / 10000, Eta: 0.375\n",
      "Epoch 33: 9705 / 10000, Eta: 0.375\n",
      "Epoch 34: 9705 / 10000, Eta: 0.375\n",
      "Epoch 35: 9763 / 10000, Eta: 0.1875\n",
      "Epoch 36: 9778 / 10000, Eta: 0.1875\n",
      "Epoch 37: 9736 / 10000, Eta: 0.1875\n",
      "Epoch 38: 9757 / 10000, Eta: 0.1875\n",
      "Epoch 39: 9723 / 10000, Eta: 0.1875\n",
      "Epoch 40: 9771 / 10000, Eta: 0.1875\n",
      "Epoch 41: 9764 / 10000, Eta: 0.1875\n",
      "Epoch 42: 9771 / 10000, Eta: 0.1875\n",
      "Epoch 43: 9759 / 10000, Eta: 0.1875\n",
      "Epoch 44: 9752 / 10000, Eta: 0.1875\n",
      "Epoch 45: 9752 / 10000, Eta: 0.1875\n",
      "Epoch 46: 9780 / 10000, Eta: 0.09375\n",
      "Epoch 47: 9780 / 10000, Eta: 0.09375\n",
      "Epoch 48: 9794 / 10000, Eta: 0.09375\n",
      "Epoch 49: 9787 / 10000, Eta: 0.09375\n",
      "Epoch 50: 9778 / 10000, Eta: 0.09375\n",
      "Epoch 51: 9781 / 10000, Eta: 0.09375\n",
      "Epoch 52: 9792 / 10000, Eta: 0.09375\n",
      "Epoch 53: 9780 / 10000, Eta: 0.09375\n",
      "Epoch 54: 9791 / 10000, Eta: 0.09375\n",
      "Epoch 55: 9788 / 10000, Eta: 0.09375\n",
      "Epoch 56: 9788 / 10000, Eta: 0.09375\n",
      "Epoch 57: 9788 / 10000, Eta: 0.09375\n",
      "Epoch 58: 9793 / 10000, Eta: 0.046875\n",
      "Epoch 59: 9794 / 10000, Eta: 0.046875\n",
      "Epoch 60: 9798 / 10000, Eta: 0.046875\n",
      "Epoch 61: 9782 / 10000, Eta: 0.046875\n",
      "Epoch 62: 9788 / 10000, Eta: 0.046875\n",
      "Epoch 63: 9791 / 10000, Eta: 0.046875\n",
      "Epoch 64: 9791 / 10000, Eta: 0.046875\n",
      "Epoch 65: 9781 / 10000, Eta: 0.046875\n",
      "Epoch 66: 9794 / 10000, Eta: 0.046875\n",
      "Epoch 67: 9805 / 10000, Eta: 0.046875\n",
      "Epoch 68: 9805 / 10000, Eta: 0.046875\n",
      "Epoch 69: 9795 / 10000, Eta: 0.0234375\n",
      "Epoch 70: 9800 / 10000, Eta: 0.0234375\n",
      "Epoch 71: 9798 / 10000, Eta: 0.0234375\n",
      "Epoch 72: 9795 / 10000, Eta: 0.0234375\n",
      "Epoch 73: 9797 / 10000, Eta: 0.0234375\n",
      "Epoch 74: 9795 / 10000, Eta: 0.0234375\n",
      "Epoch 75: 9793 / 10000, Eta: 0.0234375\n",
      "Epoch 76: 9786 / 10000, Eta: 0.0234375\n",
      "Epoch 77: 9785 / 10000, Eta: 0.0234375\n",
      "Epoch 78: 9794 / 10000, Eta: 0.0234375\n",
      "Epoch 79: 9794 / 10000, Eta: 0.0234375\n",
      "Training finished. Final classification accuracy: 97.91%\n",
      "0.9791\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'# Tests diferent etas for classification accuracy\\ndef ReLU(x):\\n    return np.maximum(x, 0)\\ndef dReLU(x):\\n    return 0.5 + 0.5*np.sign(x)\\nresults = [Network(784, 30, 10).SGD(training_data, 30, 10, 3.0, test_data=test_data, output=\"percent\", quiet=False)\\n           for i in np.arange(15)]\\nfor percent in results:\\n    # print(\"Eta: {0}, Accuracy: {1}%\".format(eta, percent*100))\\n    print(\"Accuracy: {0}%\".format(percent*100))\\nprint(\"Done\")'"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data, validation_data, test_data = load_data_wrapper()\n",
    "training_data, validation_data, test_data = list(training_data), list(validation_data), list(test_data)\n",
    "print(\"Start training networks\")\n",
    "# %timeit Network(784, 100, 10).SGD(training_data, 10, 10, 3.0, test_data=test_data, output=\"percent\", quiet=True)\n",
    "# %timeit OptNetwork(784, 100, 10).SGD(training_data, 10, 10, 3.0, test_data=test_data, output=\"percent\", quiet=True, cost=\"quadratic\")\n",
    "# resultsl2 = [(SelfNetwork2(784, 100, 10).SGD(training_data, 30, i, 4.0, \n",
    "#                                           regularization=None, test_data=test_data, \n",
    "#                                           schedule=10, output=\"percent\"), i) \n",
    "#           for i in range(1, 100)]\n",
    "print(SelfNetwork2(784, 100, 10).SGD(training_data, 30, 10, 1.5, test_data=test_data,\n",
    "                                    output=\"percent\", momentum=0.5, schedule=10))\n",
    "\"\"\"# Tests diferent etas for classification accuracy\n",
    "def ReLU(x):\n",
    "    return np.maximum(x, 0)\n",
    "def dReLU(x):\n",
    "    return 0.5 + 0.5*np.sign(x)\n",
    "results = [Network(784, 30, 10).SGD(training_data, 30, 10, 3.0, test_data=test_data, output=\"percent\", quiet=False)\n",
    "           for i in np.arange(15)]\n",
    "for percent in results:\n",
    "    # print(\"Eta: {0}, Accuracy: {1}%\".format(eta, percent*100))\n",
    "    print(\"Accuracy: {0}%\".format(percent*100))\n",
    "print(\"Done\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max accuracy: 0.9652\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a0fe4a8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 380
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "resultsl2 = np.asarray(resultsl2)\n",
    "plt.scatter(resultsl2[:11,1], resultsl2[:11,0])\n",
    "\n",
    "print(\"Max accuracy:\", np.amax(resultsl2[:11, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "mnist_loader\n",
    "~~~~~~~~~~~~\n",
    "A library to load the MNIST image data.  For details of the data\n",
    "structures that are returned, see the doc strings for ``load_data``\n",
    "and ``load_data_wrapper``.  In practice, ``load_data_wrapper`` is the\n",
    "function usually called by our neural network code.\n",
    "\"\"\"\n",
    "\n",
    "#### Libraries\n",
    "# Standard library\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Return the MNIST data as a tuple containing the training data,\n",
    "    the validation data, and the test data.\n",
    "    The ``training_data`` is returned as a tuple with two entries.\n",
    "    The first entry contains the actual training images.  This is a\n",
    "    numpy ndarray with 50,000 entries.  Each entry is, in turn, a\n",
    "    numpy ndarray with 784 values, representi mng the 28 * 28 = 784\n",
    "    pixels in a single MNIST image.\n",
    "    The second entry in the ``training_data`` tuple is a numpy ndarray\n",
    "    containing 50,000 entries.  Those entries are just the digit\n",
    "    values (0...9) for the corresponding images contained in the first\n",
    "    entry of the tuple.\n",
    "    The ``validation_data`` and ``test_data`` are similar, except\n",
    "    each contains only 10,000 images.\n",
    "    This is a nice data format, but for use in neural networks it's\n",
    "    helpful to modify the format of the ``training_data`` a little.\n",
    "    That's done in the wrapper function ``load_data_wrapper()``, see\n",
    "    below.\n",
    "    \"\"\"\n",
    "    f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def load_data_wrapper():\n",
    "    \"\"\"Return a tuple containing ``(training_data, validation_data,\n",
    "    test_data)``. Based on ``load_data``, but the format is more\n",
    "    convenient for use in our implementation of neural networks.\n",
    "    In particular, ``training_data`` is a list containing 50,000\n",
    "    2-tuples ``(x, y)``.  ``x`` is a 784-dimensional numpy.ndarray\n",
    "    containing the input image.  ``y`` is a 10-dimensional\n",
    "    numpy.ndarray representing the unit vector corresponding to the\n",
    "    correct digit for ``x``.\n",
    "    ``validation_data`` and ``test_data`` are lists containing 10,000\n",
    "    2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional\n",
    "    numpy.ndarry containing the input image, and ``y`` is the\n",
    "    corresponding classification, i.e., the digit values (integers)\n",
    "    corresponding to ``x``.\n",
    "    Obviously, this means we're using slightly different formats for\n",
    "    the training data and the validation / test data.  These formats\n",
    "    turn out to be the most convenient for use in our neural network\n",
    "    code.\"\"\"\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = zip(training_inputs, training_results)\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = zip(validation_inputs, va_d[1])\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = zip(test_inputs, te_d[1])\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
    "    position and zeroes elsewhere.  This is used to convert a digit\n",
    "    (0...9) into a corresponding desired output from the neural\n",
    "    network.\"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# modification of network.py to implement fully matrix-based\n",
    "# approach to backpropagation over a mini-batch\n",
    "\n",
    "import numpy as np\n",
    "import types\n",
    "\n",
    "def sigmoid(z):\n",
    "    return .5 * (1 + np.tanh(.5 * z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "class SelfNetwork2(object):\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "        if(callable(args[-1]) and callable(args[-2])):\n",
    "            self.activation = args[-2]\n",
    "            self.dadz = args[-1]\n",
    "            args = args[:-2]\n",
    "        else:\n",
    "            self.activation = sigmoid\n",
    "            self.dadz = sigmoid_prime\n",
    "        if(isinstance(args[0], (list,))):\n",
    "            sizes = args[0]\n",
    "        else:\n",
    "            sizes = args\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)/np.sqrt(x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        # Create a Network object net with 2 neurons in the first layer,\n",
    "        # 3 neurons in the second layer, and 1 neuron in the final\n",
    "        # layer, do\n",
    "        # net = Network([2, 3, 1])\n",
    "    \n",
    "    def setActivationFunction(self, func, dfunc):\n",
    "        self.activation = func\n",
    "        self.dadz = dfunc\n",
    "        \n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if \"a\" is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = self.activation(w @ a + b)\n",
    "        return a\n",
    "    \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None, \n",
    "            output=\"tuple\", quiet=False, cost='cross-entropy', lmbda=5.0, \n",
    "            regularization=\"L2\", schedule=None, momentum=None):\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        if(schedule==None or test_data==None):\n",
    "            for j in range(1, epochs+1):\n",
    "                np.random.shuffle(training_data)\n",
    "                mini_batches = [\n",
    "                    training_data[k:k+mini_batch_size]\n",
    "                    for k in range(0, n, mini_batch_size)]\n",
    "                for mini_batch in mini_batches:\n",
    "                    self.update_mini_batch(mini_batch, eta)\n",
    "                if(not(quiet)):\n",
    "                    if test_data:\n",
    "                        print(\"Epoch {0}: {1} / {2}\".format(\n",
    "                            j, self.evaluate(test_data), n_test))\n",
    "                    else:\n",
    "                        print (\"Epoch {0} complete\".format(j))\n",
    "        else:\n",
    "            for j in range(1, 8):\n",
    "                val_accs = [0, []]\n",
    "                for i in range(0, schedule):\n",
    "                    np.random.shuffle(training_data)\n",
    "                    mini_batches = [\n",
    "                        training_data[k:k+mini_batch_size]\n",
    "                        for k in range(0, n, mini_batch_size)]\n",
    "                    for mini_batch in mini_batches:\n",
    "                        self.update_mini_batch(mini_batch, eta, \n",
    "                                               cost, lmbda, n, regularization)\n",
    "                    val_accs[1].append(self.evaluate(test_data))\n",
    "                while True:\n",
    "                    np.random.shuffle(training_data)\n",
    "                    mini_batches = [\n",
    "                        training_data[k:k+mini_batch_size]\n",
    "                        for k in range(0, n, mini_batch_size)]\n",
    "                    for mini_batch in mini_batches:\n",
    "                        self.update_mini_batch(mini_batch, eta, \n",
    "                                               cost, lmbda, n, regularization)\n",
    "                    acc = self.evaluate(test_data)\n",
    "                    if(val_accs[1][val_accs[0]]>=acc):\n",
    "                        eta /= 2\n",
    "                        break\n",
    "                    else:\n",
    "                        val_accs[1][val_accs[0]] = acc\n",
    "                        val_accs[0] += 1\n",
    "                        if(val_accs[0]==schedule):\n",
    "                            val_accs[0]=0\n",
    "\n",
    "        if test_data:\n",
    "            if(output==\"tuple\"):\n",
    "                out = (self.evaluate(test_data), n_test)\n",
    "                print(\"Training finished. Final classification accuracy: {0}/{1}\".format(out[0], out[1]))\n",
    "            if(output==\"percent\"):\n",
    "                out = self.evaluate(test_data)/n_test\n",
    "                print(\"Training finished. Final classification accuracy: {0}%\".format(out*100))\n",
    "            return out\n",
    "    \n",
    "    def update_mini_batch(self, mini_batch, eta, cost, lmbda, n, regularization):\n",
    "        nabla_b = np.array([np.zeros(b.shape) for b in self.biases])\n",
    "        nabla_w = np.array([np.zeros(w.shape) for w in self.weights])\n",
    "        mini_batch = np.array(mini_batch)\n",
    "        mini_batch_size = mini_batch.size/2.0\n",
    "        images = np.concatenate(mini_batch[:,0], axis=1)\n",
    "        outputs = np.concatenate(mini_batch[:,1], axis=1)\n",
    "        zs = []\n",
    "        activations = [images]\n",
    "        activation = images\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            zs.append(w @ activations[-1] + b)\n",
    "            activations.append(self.activation(zs[-1]))\n",
    "        if(cost==\"quadratic\"):\n",
    "            delta = self.dCda(activations[-1],outputs)*self.dadz(zs[-1])\n",
    "        if(cost==\"cross-entropy\"):\n",
    "            delta = activations[-1] - outputs\n",
    "        nabla_b[-1] = np.sum(delta, axis=1)*eta/mini_batch_size\n",
    "        nabla_w[-1] = (delta @ activations[-2].T)*eta/mini_batch_size\n",
    "        for l in range(2, self.num_layers):\n",
    "            delta = (self.weights[-l+1].T @ delta)*self.dadz(zs[-l])\n",
    "            nabla_b[-l] = np.sum(delta, axis=1)*eta/mini_batch_size\n",
    "            nabla_w[-l] = (delta @ activations[-l-1].T)*eta/mini_batch_size\n",
    "        # The following basically changes the weights and biases by the mean of the nablas\n",
    "        if(regularization==\"L2\"):\n",
    "            self.weights = [(1-eta*lmbda/n)*w-nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        elif(regularization==\"L1\"):\n",
    "            self.weights = ([w-eta*lmbda/n*np.sign(w)-nw \n",
    "                            for w, nw in zip(self.weights, nabla_w)])\n",
    "        else:\n",
    "            self.weights = [w-nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = ([(b-nb.reshape(nb.shape[0],1))\n",
    "                       for b, nb in zip(self.biases, nabla_b)])\n",
    "    \n",
    "    def dCda(self, output_activations, y):\n",
    "        return (output_activations-y)\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        #print(self.feedforward(test_data[0][0]).shape)\n",
    "        return sum(int(x==y) for (x, y) in test_results)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "929 Âµs Â± 36.3 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)\n",
      "854 Âµs Â± 36.9 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "zeros = np.zeros([1000,1000,1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# modification of network.py to implement fully matrix-based\n",
    "# approach to backpropagation over a mini-batch\n",
    "\n",
    "import numpy as np\n",
    "import types\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "class OptNetwork(object):\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "        if(callable(args[-1]) and callable(args[-2])):\n",
    "            self.activation = args[-2]\n",
    "            self.dadz = args[-1]\n",
    "            args = args[:-2]\n",
    "        else:\n",
    "            self.activation = sigmoid\n",
    "            self.dadz = sigmoid_prime\n",
    "        if(isinstance(args[0], (list,))):\n",
    "            sizes = args[0]\n",
    "        else:\n",
    "            sizes = args\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        # Create a Network object net with 2 neurons in the first layer,\n",
    "        # 3 neurons in the second layer, and 1 neuron in the final\n",
    "        # layer, do\n",
    "        # net = Network([2, 3, 1])\n",
    "    \n",
    "    def setActivationFunction(self, func, dfunc):\n",
    "        self.activation = func\n",
    "        self.dadz = dfunc\n",
    "        \n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if \"a\" is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = self.activation(w @ a + b)\n",
    "        return a\n",
    "    \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None, output=\"tuple\", quiet=False, cost='cross-entropy'):\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in range(1, epochs+1):\n",
    "            np.random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta, cost)\n",
    "            if(not(quiet)):\n",
    "                if test_data:\n",
    "                    print(\"Epoch {0}: {1} / {2}\".format(\n",
    "                        j, self.evaluate(test_data), n_test))\n",
    "                else:\n",
    "                    print (\"Epoch {0} complete\".format(j))\n",
    "        if test_data:\n",
    "            if(output==\"tuple\"):\n",
    "                out = (self.evaluate(test_data), n_test)\n",
    "                print(\"Training finished. Final classification accuracy: {0}/{1}\".format(out[0], out[1]))\n",
    "            if(output==\"percent\"):\n",
    "                out = self.evaluate(test_data)/n_test\n",
    "                print(\"Training finished. Final classification accuracy: {0}%\".format(out*100))\n",
    "            return out\n",
    "    \n",
    "    def update_mini_batch(self, mini_batch, eta, cost):\n",
    "        nabla_b = np.array([np.zeros(b.shape) for b in self.biases])\n",
    "        nabla_w = np.array([np.zeros(w.shape) for w in self.weights])\n",
    "        mini_batch = np.array(mini_batch)\n",
    "        mini_batch_size = mini_batch.size/2.0\n",
    "        images = np.concatenate(mini_batch[:,0], axis=1)\n",
    "        outputs = np.concatenate(mini_batch[:,1], axis=1)\n",
    "        zs = []\n",
    "        activations = [images]\n",
    "        activation = images\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            zs.append(w @ activations[-1] + b)\n",
    "            activations.append(self.activation(zs[-1]))\n",
    "        if(cost==\"quadratic\"):\n",
    "            delta = self.dCda(activations[-1],outputs)*self.dadz(zs[-1])\n",
    "        if(cost==\"cross-entropy\"):\n",
    "            delta = activations[-1] - outputs\n",
    "        nabla_b[-1] = np.sum(delta, axis=1)*eta/mini_batch_size\n",
    "        nabla_w[-1] = (delta @ activations[-2].T)*eta/mini_batch_size\n",
    "        for l in range(2, self.num_layers):\n",
    "            delta = (self.weights[-l+1].T @ delta)*self.dadz(zs[-l])\n",
    "            nabla_b[-l] = np.sum(delta, axis=1)*eta/mini_batch_size\n",
    "            nabla_w[-l] = (delta @ activations[-l-1].T)*eta/mini_batch_size\n",
    "        # The following basically changes the weights and biases by the mean of the nablas\n",
    "        self.weights = [w-n for w, n in zip(self.weights, nabla_w)]\n",
    "        self.biases = [(b-n.reshape(n.shape[0],1)) for b, n in zip(self.biases, nabla_b)]\n",
    "        \n",
    "    def backprop(self, x, y):\n",
    "        # Not used since OptNetwork's backprop is all in update_mini_batch\n",
    "        # x is the input, y is the desired output\n",
    "        # sets shape for nabla_b and nabla_w\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward lists\n",
    "        activation = x\n",
    "        activations = [x]\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        # actual feedforward loop, saving the z matrices\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = w @ activation + b\n",
    "            zs.append(z)\n",
    "            activation = self.activation(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        # initializes lists for backprop\n",
    "        delta = self.dCda(activations[-1], y) * self.dadz(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = delta @ activations[-2].T\n",
    "        # The meat of backprop!\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = self.dadz(z)\n",
    "            delta = (self.weights[-l+1].T @ delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = delta @ activations[-l-1].T\n",
    "        return (nabla_b, nabla_w)\n",
    "    \n",
    "    def dCda(self, output_activations, y):\n",
    "        return (output_activations-y)\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        #print(self.feedforward(test_data[0][0]).shape)\n",
    "        return sum(int(x==y) for (x, y) in test_results)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# network.py in NN&DL\n",
    "\n",
    "import numpy as np\n",
    "import types\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "class Network(object):\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "        if(callable(args[-1]) and callable(args[-2])):\n",
    "            self.activation = args[-2]\n",
    "            self.dactivation = args[-1]\n",
    "            args = args[:-2]\n",
    "        else:\n",
    "            self.activation = sigmoid\n",
    "            self.dactivation = sigmoid_prime\n",
    "        if(isinstance(args[0], (list,))):\n",
    "            sizes = args[0]\n",
    "        else:\n",
    "            sizes = args\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        # Create a Network object net with 2 neurons in the first layer,\n",
    "        # 3 neurons in the second layer, and 1 neuron in the final\n",
    "        # layer, do\n",
    "        # net = Network([2, 3, 1])\n",
    "    \n",
    "    def setActivationFunction(self, func, dfunc):\n",
    "        self.activation = func\n",
    "        self.dactivation = dfunc\n",
    "        \n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if \"a\" is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            print(\"a\", a.shape)\n",
    "            print(\"w\", w.shape)\n",
    "            print(\"b\", b.shape)\n",
    "            print()\n",
    "            a = self.activation(w @ a + b)\n",
    "        return a\n",
    "    \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None, output=\"tuple\", quiet=False):\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in range(1, epochs+1):\n",
    "            np.random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if(not(quiet)):\n",
    "                if test_data:\n",
    "                    print(\"Epoch {0}: {1} / {2}\".format(\n",
    "                        j, self.evaluate(test_data), n_test))\n",
    "                else:\n",
    "                    print (\"Epoch {0} complete\".format(j))\n",
    "        if test_data:\n",
    "            if(output==\"tuple\"):\n",
    "                out = (self.evaluate(test_data), n_test)\n",
    "                print(\"Training finished. Final classification accuracy: {0}/{1}\".format(out[0], out[1]))\n",
    "            if(output==\"percent\"):\n",
    "                out = self.evaluate(test_data)/n_test\n",
    "                print(\"Training finished. Final classification accuracy: {0}%\".format(out*100))\n",
    "            return out\n",
    "    \n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        # JZ: The following basically changes the weights and biases by the mean of the nablas\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                      for b, nb in zip(self.biases, nabla_b)]\n",
    "        \n",
    "    def backprop(self, x, y):\n",
    "        # x is the input, y is the desired output\n",
    "        # sets shape for nabla_b and nabla_w\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward lists\n",
    "        activation = x\n",
    "        activations = [x]\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        # actual feedforward loop, saving the z matrices\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = w @ activation + b\n",
    "            zs.append(z)\n",
    "            activation = self.activation(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        # initializes lists for backprop\n",
    "        delta = self.cost_derivative(activations[-1], y) * self.dactivation(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = delta @ activations[-2].T\n",
    "        # The meat of backprop!\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = self.dactivation(z)\n",
    "            delta = (self.weights[-l+1].T @ delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = delta @ activations[-l-1].T\n",
    "        return (nabla_b, nabla_w)\n",
    "    \n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        return (output_activations-y)\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        print(self.feedforward(test_data[0][0]).shape)\n",
    "        print(self.weights[-2].shape)\n",
    "        return sum(int(x==y) for (x, y) in test_results)\n",
    "\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
